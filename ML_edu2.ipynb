{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Edu2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hinkim/Machine_Learning/blob/master/ML_edu2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSSHrKTXt1Iw",
        "colab_type": "text"
      },
      "source": [
        "## Logistic classification\n",
        "\n",
        "-> linear regresion으로 쓰면 문제가 있음\n",
        "\n",
        "-> 0과 1로만 분류하기 때문에 아무리 큰 값이 들어와도 y는 1보다 클 수 없음\n",
        "\n",
        "-> 0 ~ 1 사이로 출력할 수 있는 함수가 필요함\n",
        "\n",
        "-> sigmoid function (logistic function)\n",
        "\n",
        "## Cost function\n",
        "\n",
        "-> linear regression으로 가설을 세웠을 때의 cost function 같이 2차원 모양의 곡선이 아님\n",
        "\n",
        "-> 울퉁불퉁한 곡선의 형태기 때문에 시작하는 점에 따라서 global minimum이 아니라 local minimum을 찾을 수 있음\n",
        "\n",
        "-> 따라서 gradient descent algorithm 사용 불가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwLiVFKVuSxe",
        "colab_type": "text"
      },
      "source": [
        "### Cost값의 의미\n",
        "-> 실제 값과 예측한 값이 같으면 or 비슷하면 cost값은 작아지고, 예측한 값이 틀리면 cost값은 커짐\n",
        "\n",
        "<img width=\"297\" alt=\"Logistic_cost\" src=\"https://user-images.githubusercontent.com/51468250/67644361-37d0fe00-f964-11e9-877a-2223474fc668.PNG\">\n",
        "\n",
        "-> -ylog(H(x)) - (1-y)log(1-H(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEYY5S70tyjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lab 5 Logistic Regression Classifier\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "x_data = [[1, 2],\n",
        "          [2, 3],\n",
        "          [3, 1],\n",
        "          [4, 3],\n",
        "          [5, 3],\n",
        "          [6, 2]]\n",
        "y_data = [[0],\n",
        "          [0],\n",
        "          [0],\n",
        "          [1],\n",
        "          [1],\n",
        "          [1]]\n",
        "\n",
        "# placeholders for a tensor that will be always fed.\n",
        "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "\n",
        "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
        "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0QxrAxuxCYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d586e0a-5b1e-407f-b8a0-6ecebe9fd60b"
      },
      "source": [
        "# cost/loss function\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
        "                       tf.log(1 - hypothesis))\n",
        "\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "# hypothesis값이 0.5보다 크면 true, 아니면 false로 나타낼텐데 float32 type으로\n",
        "# cast하면 1 or 0으로 나타낼 수 있음\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10001):\n",
        "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 200 == 0:\n",
        "            print(step, cost_val)\n",
        "\n",
        "    # Accuracy report\n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
        "                       feed_dict={X: x_data, Y: y_data})\n",
        "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.84807867\n",
            "200 0.58504885\n",
            "400 0.55539876\n",
            "600 0.53128415\n",
            "800 0.51020116\n",
            "1000 0.49101636\n",
            "1200 0.4731789\n",
            "1400 0.45640385\n",
            "1600 0.44053388\n",
            "1800 0.42547455\n",
            "2000 0.41116396\n",
            "2200 0.39755607\n",
            "2400 0.38461316\n",
            "2600 0.3723019\n",
            "2800 0.36059117\n",
            "3000 0.3494512\n",
            "3200 0.33885324\n",
            "3400 0.32876936\n",
            "3600 0.31917262\n",
            "3800 0.31003675\n",
            "4000 0.3013368\n",
            "4200 0.29304847\n",
            "4400 0.28514892\n",
            "4600 0.27761635\n",
            "4800 0.27043003\n",
            "5000 0.26357028\n",
            "5200 0.25701866\n",
            "5400 0.25075755\n",
            "5600 0.24477072\n",
            "5800 0.2390427\n",
            "6000 0.2335589\n",
            "6200 0.22830574\n",
            "6400 0.22327043\n",
            "6600 0.218441\n",
            "6800 0.21380605\n",
            "7000 0.20935528\n",
            "7200 0.2050788\n",
            "7400 0.20096724\n",
            "7600 0.19701199\n",
            "7800 0.19320484\n",
            "8000 0.18953824\n",
            "8200 0.18600498\n",
            "8400 0.1825984\n",
            "8600 0.17931205\n",
            "8800 0.17614006\n",
            "9000 0.17307681\n",
            "9200 0.17011712\n",
            "9400 0.16725604\n",
            "9600 0.16448902\n",
            "9800 0.16181149\n",
            "10000 0.15921949\n",
            "\n",
            "Hypothesis:  [[0.03493407]\n",
            " [0.16416825]\n",
            " [0.32434478]\n",
            " [0.7726206 ]\n",
            " [0.9339197 ]\n",
            " [0.9782899 ]] \n",
            "Correct (Y):  [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]] \n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJilonh40wrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2dc2744e-3662-444d-8281-4732d8aab9f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBId5ZxR1abX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "18d0b863-2de6-485c-de5c-c1da40556b0f"
      },
      "source": [
        "!ls \"/gdrive/My Drive\" | grep data*"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data-03-diabetes.csv\n",
            "data-04-zoo.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0FdIxb_zAqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0918efd1-4780-4578-d40b-1a9aebd2cded"
      },
      "source": [
        "# Lab 5 Logistic Regression Classifier\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "xy = np.loadtxt('/gdrive/My Drive/data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "print(x_data.shape, y_data.shape)\n",
        "\n",
        "# placeholders for a tensor that will be always fed.\n",
        "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(-tf.matmul(X, W)))\n",
        "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(759, 8) (759, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skjHvuDzz0YX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "829672fe-0371-49fc-db65-43ba0f285dfb"
      },
      "source": [
        "# cost/loss function\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
        "                       tf.log(1 - hypothesis))\n",
        "\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10001):\n",
        "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 1000 == 0:\n",
        "            print(step, cost_val)\n",
        "\n",
        "    # Accuracy report\n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
        "                       feed_dict={X: x_data, Y: y_data})\n",
        "    print(\"\\nAccuracy: \", a)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.6161271\n",
            "1000 0.56596977\n",
            "2000 0.537397\n",
            "3000 0.5191828\n",
            "4000 0.5070188\n",
            "5000 0.4985801\n",
            "6000 0.4925432\n",
            "7000 0.4881148\n",
            "8000 0.48479846\n",
            "9000 0.48227128\n",
            "10000 0.4803167\n",
            "\n",
            "Accuracy:  0.77602106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8BJgUBX3jhg",
        "colab_type": "text"
      },
      "source": [
        "## Softmax classification\n",
        "##### 데이터를 2개 이상의 그룹으로 나누기 위해 binary classification (Logistic Regression)을 확장한 모델\n",
        "\n",
        "Multinomial classification\n",
        "\n",
        "<img width=\"443\" alt=\"Softmax_concept\" src=\"https://user-images.githubusercontent.com/51468250/67645291-1fb0ad00-f96b-11e9-9cae-bb439c6f549d.PNG\">\n",
        "\n",
        "각각 독립된 3개의 클래스를 가지고 동작\n",
        "\n",
        "X -> A -> Y' = A인지 아닌지만 확인 <br>\n",
        "X -> B -> Y' = B인지 아닌지만 확인 <br>\n",
        "X -> C -> Y' = C인지 아닌지만 확인\n",
        "\n",
        "<img width=\"494\" alt=\"Softmax_concept2\" src=\"https://user-images.githubusercontent.com/51468250/67645366-b67d6980-f96b-11e9-8d30-eee0a9ac6ca6.PNG\">\n",
        "\n",
        "\n",
        "## Cost function\n",
        "\n",
        "cross-entropy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCJa3_rY_Fvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lab 6 Softmax Classifier\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "x_data = [[1, 2, 1, 1],\n",
        "          [2, 1, 3, 2],\n",
        "          [3, 1, 3, 4],\n",
        "          [4, 1, 5, 5],\n",
        "          [1, 7, 5, 5],\n",
        "          [1, 2, 5, 6],\n",
        "          [1, 6, 6, 6],\n",
        "          [1, 7, 7, 7]]\n",
        "y_data = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, 4])\n",
        "Y = tf.placeholder(\"float\", [None, 3])\n",
        "nb_classes = 3\n",
        "\n",
        "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "\n",
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69R-zzxV_lp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "cff499f2-8658-4bc2-d1e6-2312e9f6ded6"
      },
      "source": [
        "# Cross entropy cost/loss\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(2001):\n",
        "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n",
        "\n",
        "            if step % 200 == 0:\n",
        "                print(step, cost_val)\n",
        "\n",
        "    print('--------------')\n",
        "    # Testing & One-hot encoding\n",
        "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
        "    print(a, sess.run(tf.argmax(a, 1)))\n",
        "\n",
        "    print('--------------')\n",
        "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
        "    print(b, sess.run(tf.argmax(b, 1)))\n",
        "\n",
        "    print('--------------')\n",
        "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
        "    print(c, sess.run(tf.argmax(c, 1)))\n",
        "\n",
        "    print('--------------')\n",
        "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
        "    print(all, sess.run(tf.argmax(all, 1)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 9.055746\n",
            "200 0.5518613\n",
            "400 0.44513813\n",
            "600 0.35089633\n",
            "800 0.26192346\n",
            "1000 0.22470412\n",
            "1200 0.20409891\n",
            "1400 0.1868866\n",
            "1600 0.17228425\n",
            "1800 0.15974122\n",
            "2000 0.14885426\n",
            "--------------\n",
            "[[1.9264005e-03 9.9806339e-01 1.0254445e-05]] [1]\n",
            "--------------\n",
            "[[0.9115941  0.07665096 0.01175495]] [0]\n",
            "--------------\n",
            "[[1.0441232e-08 3.1730966e-04 9.9968266e-01]] [2]\n",
            "--------------\n",
            "[[1.9264014e-03 9.9806339e-01 1.0254435e-05]\n",
            " [9.1159415e-01 7.6650955e-02 1.1754937e-02]\n",
            " [1.0441233e-08 3.1730966e-04 9.9968266e-01]] [1 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDKRxsqMCMjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d2c0ff8d-2242-48de-97eb-fa82d7649bc2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "# Predicting animal type based on various features\n",
        "xy = np.loadtxt('/gdrive/My Drive/data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "print(x_data.shape, y_data.shape)\n",
        "\n",
        "'''\n",
        "(101, 16) (101, 1)\n",
        "'''\n",
        "\n",
        "nb_classes = 7  # 0 ~ 6\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 16])\n",
        "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n",
        "\n",
        "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
        "print(\"one_hot:\", Y_one_hot)\n",
        "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
        "print(\"reshape one_hot:\", Y_one_hot)\n",
        "\n",
        "'''\n",
        "one_hot: Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
        "reshape one_hot: Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
        "'''\n",
        "\n",
        "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "\n",
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
        "logits = tf.matmul(X, W) + b\n",
        "hypothesis = tf.nn.softmax(logits)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101, 16) (101, 1)\n",
            "one_hot: Tensor(\"one_hot_1:0\", shape=(?, 1, 7), dtype=float32)\n",
            "reshape one_hot: Tensor(\"Reshape_1:0\", shape=(?, 7), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgnF9aogFDml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "403971fd-2749-4e9f-8839-87d0b151511d"
      },
      "source": [
        "# Cross entropy cost/loss\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
        "                                                                 labels=tf.stop_gradient([Y_one_hot])))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "prediction = tf.argmax(hypothesis, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(2001):\n",
        "        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "                                        \n",
        "        if step % 200 == 0:\n",
        "            print(\"Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}\".format(step, cost_val, acc_val))\n",
        "\n",
        "    # Let's see if we can predict\n",
        "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
        "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
        "    for p, y in zip(pred, y_data.flatten()):\n",
        "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:     0\tCost: 11.225\tAcc: 11.88%\n",
            "Step:   200\tCost: 0.327\tAcc: 91.09%\n",
            "Step:   400\tCost: 0.198\tAcc: 97.03%\n",
            "Step:   600\tCost: 0.146\tAcc: 97.03%\n",
            "Step:   800\tCost: 0.117\tAcc: 100.00%\n",
            "Step:  1000\tCost: 0.098\tAcc: 100.00%\n",
            "Step:  1200\tCost: 0.084\tAcc: 100.00%\n",
            "Step:  1400\tCost: 0.074\tAcc: 100.00%\n",
            "Step:  1600\tCost: 0.066\tAcc: 100.00%\n",
            "Step:  1800\tCost: 0.059\tAcc: 100.00%\n",
            "Step:  2000\tCost: 0.054\tAcc: 100.00%\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2rBSkp2Hhcp",
        "colab_type": "text"
      },
      "source": [
        "## Tips\n",
        "\n",
        "### Learning rate\n",
        "common: 0.01, cost값이 발산하게 되면 줄이고, cost값이 너무 천천히 작아진다면 크게 조정\n",
        "\n",
        "### Data preprocessing\n",
        "normalized data: 값의 범위가 어떤 범위 안에 들어가도록 조정 (데이터 중에 차이가 크게 나는 것이 있는지 확인) \n",
        "\n",
        "### Overfitting\n",
        "가지고 있는 데이터에만 맞춰져 있는 모델이라, 실제적으로 사용할 때는 정확도가 떨어질 수 있음 <br>\n",
        "-> More training data <br>\n",
        "-> Reduce the number of features <br>\n",
        "-> Regularization\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLw3ecgDT49y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lab 7 Learning rate and Evaluation\n",
        "# Normal learning rate\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "x_data = [[1, 2, 1],\n",
        "          [1, 3, 2],\n",
        "          [1, 3, 4],\n",
        "          [1, 5, 5],\n",
        "          [1, 7, 5],\n",
        "          [1, 2, 5],\n",
        "          [1, 6, 6],\n",
        "          [1, 7, 7]]\n",
        "y_data = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "# Evaluation our model using this test dataset\n",
        "x_test = [[2, 1, 1],\n",
        "          [3, 1, 2],\n",
        "          [3, 3, 4]]\n",
        "y_test = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1]]\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, 3])\n",
        "Y = tf.placeholder(\"float\", [None, 3])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([3, 3]))\n",
        "b = tf.Variable(tf.random_normal([3]))\n",
        "\n",
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_sgoIPCT-tN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "39e58c17-da89-461c-b0dd-9f8cbb757d48"
      },
      "source": [
        "# Cross entropy cost/loss\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "# Try to change learning_rate to small numbers\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Correct prediction Test model\n",
        "prediction = tf.argmax(hypothesis, 1)\n",
        "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(201):\n",
        "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 20 == 0:\n",
        "          print(step, cost_val, W_val)\n",
        "\n",
        "    # predict\n",
        "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
        "    # Calculate the accuracy\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2.1444025 [[-0.6453531  -1.4070444   1.1613661 ]\n",
            " [ 0.69393617  1.640023    0.7581979 ]\n",
            " [ 0.53586733 -0.38953224 -1.1654013 ]]\n",
            "20 0.82288337 [[-0.89059854 -1.415377    1.4149445 ]\n",
            " [ 0.8119314   1.1885427   1.0916829 ]\n",
            " [ 0.09490148 -0.28896475 -0.82500273]]\n",
            "40 0.7345104 [[-1.0484712  -1.3928951   1.5503352 ]\n",
            " [ 0.9855602   1.0551983   1.0513989 ]\n",
            " [-0.01264633 -0.15775906 -0.84866047]]\n",
            "60 0.69320047 [[-1.1800117  -1.3816211   1.6706018 ]\n",
            " [ 1.0712421   0.999482    1.0214337 ]\n",
            " [-0.04597051 -0.10175525 -0.8713401 ]]\n",
            "80 0.66570395 [[-1.2953715  -1.3745397   1.7788804 ]\n",
            " [ 1.1161827   0.9793133   0.9966619 ]\n",
            " [-0.04564597 -0.0799362  -0.89348376]]\n",
            "100 0.6438613 [[-1.4000415  -1.3684808   1.8774914 ]\n",
            " [ 1.1416913   0.97436255  0.9761042 ]\n",
            " [-0.03016286 -0.07308695 -0.915816  ]]\n",
            "120 0.6252973 [[-1.4970335  -1.3622104   1.968213  ]\n",
            " [ 1.15717     0.97576636  0.95922226]\n",
            " [-0.00766925 -0.07286084 -0.9385357 ]]\n",
            "140 0.60902494 [[-1.5881504  -1.3553007   2.0524201 ]\n",
            " [ 1.167076    0.979656    0.94542676]\n",
            " [ 0.01805624 -0.07557663 -0.9615452 ]]\n",
            "160 0.5944947 [[-1.6745611  -1.3476552   2.1311848 ]\n",
            " [ 1.1736801   0.98434395  0.93413454]\n",
            " [ 0.04517446 -0.07958531 -0.9846546 ]]\n",
            "180 0.5813462 [[-1.7570723  -1.3393103   2.2053514 ]\n",
            " [ 1.1782142   0.9891116   0.9248333 ]\n",
            " [ 0.07275787 -0.08414292 -1.0076804 ]]\n",
            "200 0.56932366 [[-1.836269   -1.3303516   2.275589  ]\n",
            " [ 1.181385    0.99367136  0.91710305]\n",
            " [ 0.1003271  -0.08891293 -1.0304797 ]]\n",
            "Prediction: [2 2 2]\n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwIKJ9VxUodV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "6210e867-09ff-42b6-a6fb-f131ef5b324e"
      },
      "source": [
        "# Big learning rate\n",
        "# Cross entropy cost/loss\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "# Try to change learning_rate to small numbers\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.7).minimize(cost)\n",
        "\n",
        "# Correct prediction Test model\n",
        "prediction = tf.argmax(hypothesis, 1)\n",
        "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10):\n",
        "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
        "        print(step, cost_val, W_val)\n",
        "\n",
        "    # predict\n",
        "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
        "    # Calculate the accuracy\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 4.748129 [[ 0.21938549  0.38984972  3.144104  ]\n",
            " [-4.092165    1.7296969   0.82555515]\n",
            " [-2.6020536   4.08962    -0.08477351]]\n",
            "1 inf [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "2 nan [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "3 nan [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "4 nan [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "5 nan [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "6 nan [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "7 nan [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "8 nan [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "9 nan [[nan nan nan]\n",
            " [nan nan nan]\n",
            " [nan nan nan]]\n",
            "Prediction: [0 0 0]\n",
            "Accuracy:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73u0PTcIU9xF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "5ed71174-a36d-4473-8c08-78b3aaec014b"
      },
      "source": [
        "# Small learning rate\n",
        "# Cross entropy cost/loss\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "# Try to change learning_rate to small numbers\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
        "\n",
        "# Correct prediction Test model\n",
        "prediction = tf.argmax(hypothesis, 1)\n",
        "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(201):\n",
        "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 20 == 0:\n",
        "          print(step, cost_val, W_val)\n",
        "\n",
        "    # predict\n",
        "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
        "    # Calculate the accuracy\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "20 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "40 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "60 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "80 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "100 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "120 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "140 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "160 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "180 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "200 2.1444025 [[-0.6263353  -1.3909553   1.1262592 ]\n",
            " [ 0.6991377   1.7296646   0.6633548 ]\n",
            " [ 0.57651556 -0.34551203 -1.2500697 ]]\n",
            "Prediction: [2 2 0]\n",
            "Accuracy:  0.6666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5B1iVD8ukiu",
        "colab_type": "text"
      },
      "source": [
        "### MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be4CrDBQucLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "61820b1d-8f0e-4899-9fce-c2f188fc632e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "nb_classes = 10\n",
        "\n",
        "# MNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 0 - 9 digits recognition = 10 classes\n",
        "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
        "b = tf.Variable(tf.random_normal([nb_classes]))\n",
        "\n",
        "# Hypothesis (using softmax)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKYEGgQtvGOf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "6ed82b70-c734-4d0e-dd24-4d95d652930f"
      },
      "source": [
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Test model\n",
        "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# parameters\n",
        "num_epochs = 20\n",
        "batch_size = 100\n",
        "num_iterations = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training cycle\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test the model using test sets\n",
        "    # sess.run() == eval()\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval(\n",
        "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, mnist.test.num_examples - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001, Cost: 2.672426570\n",
            "Epoch: 0002, Cost: 1.076048675\n",
            "Epoch: 0003, Cost: 0.868284545\n",
            "Epoch: 0004, Cost: 0.767021303\n",
            "Epoch: 0005, Cost: 0.703292290\n",
            "Epoch: 0006, Cost: 0.657308690\n",
            "Epoch: 0007, Cost: 0.621956688\n",
            "Epoch: 0008, Cost: 0.593385604\n",
            "Epoch: 0009, Cost: 0.569349916\n",
            "Epoch: 0010, Cost: 0.549566229\n",
            "Epoch: 0011, Cost: 0.532382537\n",
            "Epoch: 0012, Cost: 0.517615450\n",
            "Epoch: 0013, Cost: 0.503979745\n",
            "Epoch: 0014, Cost: 0.492164900\n",
            "Epoch: 0015, Cost: 0.480872580\n",
            "Epoch: 0016, Cost: 0.471145752\n",
            "Epoch: 0017, Cost: 0.462535726\n",
            "Epoch: 0018, Cost: 0.453660646\n",
            "Epoch: 0019, Cost: 0.446642911\n",
            "Epoch: 0020, Cost: 0.439076144\n",
            "Learning finished\n",
            "Accuracy:  0.8923\n",
            "Label:  [9]\n",
            "Prediction:  [9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
